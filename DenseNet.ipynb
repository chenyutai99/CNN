{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initial Dense block is structured with Batch Normalization, ReLU, and 3x3 convolutional layers.\n",
    "class basic_layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(basic_layer, self).__init__()      \n",
    "        self.basic = nn.Sequential(nn.BatchNorm2d(in_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.basic(x)\n",
    "        out = torch.cat([out, x], dim=1) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottleneck (Batch Normalization, ReLU, 1x1 convolutional layer, Batch Normalization, ReLU, 3x3 convolutional layer) is used in order to reduce the channel dimensionality and the amount of parameters calculated.\n",
    "class bottleneck_layer(nn.Module):\n",
    "    def __init__(self, in_channels, bottleneck_size, growth_rate, drop_rate):\n",
    "        super(bottleneck_layer, self).__init__()      \n",
    "        self.bottleneck = nn.Sequential(nn.BatchNorm2d(in_channels),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.Conv2d(in_channels=in_channels, out_channels=bottleneck_size*growth_rate, kernel_size=1, padding=0, bias=False),\n",
    "                      nn.BatchNorm2d(bottleneck_size*growth_rate),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.Conv2d(in_channels=bottleneck_size*growth_rate, out_channels=growth_rate, kernel_size=3, padding=1, bias=False))\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.dropout = nn.Dropout(p=self.drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bottleneck(x)\n",
    "        if self.drop_rate > 0:\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = torch.cat([out, x], dim=1) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, out_channels, growth_rate, num_layers, num_classes):\n",
    "        super(DenseNet, self).__init__()      \n",
    "        bottleneck_size = 4\n",
    "        drop_rate = 0\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=out_channels, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(inplace=True))\n",
    "\n",
    "        self.maxpooling = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        layers = []\n",
    "        block_in_channels = out_channels\n",
    "        for i, num_layer in enumerate(num_layers):\n",
    "        layers.append(self.dense_block(block_in_channels, bottleneck_size, growth_rate, drop_rate, num_layer))\n",
    "        block_in_channels += num_layer*growth_rate\n",
    "\n",
    "        if i != len(num_layers)-1:\n",
    "          layers.append(self.transition_layer(block_in_channels, block_in_channels // 2))\n",
    "          block_in_channels = block_in_channels // 2\n",
    "\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.bn = nn.BatchNorm2d(block_in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.GAP_pooling = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(block_in_channels, num_classes)\n",
    "\n",
    "    def dense_block(self, in_channels, bottleneck_size, growth_rate, drop_rate, num_layers):\n",
    "        block = []\n",
    "        for i in range(num_layers):\n",
    "        block.append(bottleneck_layer(in_channels + i*growth_rate, bottleneck_size, growth_rate, drop_rate))\n",
    "\n",
    "        return nn.Sequential(*block)\n",
    "\n",
    "    def transition_layer(self, in_channels, out_channels):\n",
    "        transition = nn.Sequential(nn.BatchNorm2d(in_channels),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, padding=0, bias=False),\n",
    "                      nn.AvgPool2d(kernel_size=2,stride=2))\n",
    "\n",
    "        return transition\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpooling(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.GAP_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNet_n(num_layers):\n",
    "    if num_layers == 121:\n",
    "        # DenseNet121\n",
    "        model = DenseNet(64, 32, [6, 12, 24, 16], num_classes)\n",
    "\n",
    "    elif num_layers == 169:\n",
    "        # DenseNet169\n",
    "        model = DenseNet(64, 32, [6, 12, 32, 32], num_classes)\n",
    "\n",
    "    elif num_layers == 201:\n",
    "        # DenseNet201\n",
    "        model = DenseNet(64, 32, [6, 12, 48, 32], num_classes)\n",
    "\n",
    "    elif num_layers == 101:\n",
    "        # DenseNet\n",
    "        model = DenseNet(64, 32, [6, 12, 64, 48], num_classes)\n",
    "\n",
    "    else:\n",
    "        print(\"error\")\n",
    "\n",
    "        return\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "num_classes=2\n",
    "\n",
    "# DenseNet121\n",
    "# model = DenseNet(64, 32, [6, 12, 24, 16], num_classes)\n",
    "\n",
    "# DenseNet169\n",
    "model = DenseNet_n(169)\n",
    "\n",
    "if CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# kaggle api\n",
    "api_token = {\"username\":\"aaa\",\"key\":\"kkk\"}\n",
    " \n",
    "if not os.path.exists(\"/root/.kaggle\"):\n",
    "    os.makedirs(\"/root/.kaggle\")\n",
    "\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    " \n",
    "if not os.path.exists(\"/kaggle\"):\n",
    "    os.makedirs(\"/kaggle\")\n",
    "os.chdir('/kaggle')\n",
    "!kaggle datasets download -d chetankv/dogs-cats-images --force\n",
    " \n",
    "!ls /kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip dogs-cats-images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "transform = transforms.Compose(\n",
    "                [transforms.Resize(size=(227,227)),\n",
    "                 transforms.CenterCrop(224),\n",
    "                 transforms.RandomRotation(20),\n",
    "                  transforms.RandomHorizontalFlip(),\n",
    "                 transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,)),]\n",
    "                )\n",
    "\n",
    "# Data\n",
    "train_dataset = datasets.ImageFolder(root='/kaggle/dataset/training_set', transform=transform)\n",
    "valid_dataset = datasets.ImageFolder(root='/kaggle/dataset/test_set', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target) \n",
    "        \n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        # clear gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        output = model(data) \n",
    "        loss = criterion(output, target) \n",
    "\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        total_train += len(target)\n",
    "        correct_train += sum((predicted == target).float())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Train Epoch: {}/{} [iter： {}/{}], acc： {:.6f}, loss： {:.6f}\".format(\n",
    "               epoch+1, num_epochs, batch_idx+1, len(train_loader),\n",
    "               correct_train / float((batch_idx + 1) * batch_size),\n",
    "               train_loss / float((batch_idx + 1) * batch_size)))\n",
    "            \n",
    "    train_acc_ = 100 * (correct_train / float(total_train))\n",
    "    train_loss_ = train_loss / total_train\n",
    "                    \n",
    "    return train_acc_, train_loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_loader, model, criterion, epoch): \n",
    "    model.eval()\n",
    "    total_valid = 0\n",
    "    correct_valid = 0\n",
    "    valid_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "        data, target = Variable(data), Variable(target) \n",
    "        \n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target) \n",
    "\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        total_valid += len(target)\n",
    "        correct_valid += sum((predicted == target).float())\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Valid Epoch: {}/{} [iter： {}/{}], acc： {:.6f}, loss： {:.6f}\".format(\n",
    "               epoch+1, num_epochs, batch_idx+1, len(valid_loader),\n",
    "               correct_valid / float((batch_idx + 1) * batch_size),\n",
    "               valid_loss / float((batch_idx + 1) * batch_size)))\n",
    "            \n",
    "    valid_acc_ = 100 * (correct_valid / float(total_valid))\n",
    "    valid_loss_ = valid_loss / total_valid\n",
    "                    \n",
    "    return valid_acc_, valid_loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, criterion, optimizer, train_loader, valid_loader):\n",
    "    # set objects for storing metrics\n",
    "    total_train_loss = []\n",
    "    total_valid_loss = []\n",
    "    total_train_accuracy = []\n",
    "    total_valid_accuracy = []\n",
    " \n",
    "    # Train model\n",
    "    for epoch in range(num_epochs):\n",
    "        # training\n",
    "        train_acc_, train_loss_ = train(train_loader, model, criterion, optimizer, epoch)\n",
    "        total_train_loss.append(train_loss_)\n",
    "        total_train_accuracy.append(train_acc_)\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            valid_acc_, valid_loss_ = validate(valid_loader, model, criterion, epoch)\n",
    "            total_valid_loss.append(valid_loss_)\n",
    "            total_valid_accuracy.append(valid_acc_)\n",
    "\n",
    "        print('==========================================================================')\n",
    "        print(\"Epoch: {}/{}， Train acc： {:.6f}， Train loss： {:.6f}， Valid acc： {:.6f}， Valid loss： {:.6f}\".format(\n",
    "               epoch+1, num_epochs, \n",
    "               train_acc_, train_loss_,\n",
    "               valid_acc_, valid_loss_))\n",
    "        print('==========================================================================')\n",
    "\n",
    "    print(\"====== END ==========\")\n",
    "\n",
    "    return total_train_loss, total_valid_loss, total_train_accuracy, total_valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_loss, total_valid_loss, total_train_accuracy, total_valid_accuracy = training_loop(model, criterion, optimizer, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(total_train, total_valid, label):\n",
    "    plt.plot(range(num_epochs), total_train, 'b-', label=f'Training_{label}')\n",
    "    plt.plot(range(num_epochs), total_valid, 'g-', label=f'validation_{label}')\n",
    "    plt.title(f'Training & Validation {label}')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel(f'{label}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(total_train_loss, total_valid_loss, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(total_train_accuracy, total_valid_accuracy, 'accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
